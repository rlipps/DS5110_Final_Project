{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad1572d-0d1d-4cbe-950d-7e483b99e756",
   "metadata": {},
   "source": [
    "# Spark Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd92df-61b9-47f2-906b-9e6307e42871",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0fc9ef-ea49-42ae-91c9-322765f378d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e111679d-e030-470a-835b-f452687bb488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import findspark\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d81c18-4eaf-4605-a1f1-07c310c0f4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/21 23:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The entry point into all functionality in Spark is the SparkSession class.\n",
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110/CS5501: my awesome Spark program\")\n",
    "\t.master(\"spark://172.31.88.97:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"1024M\")\n",
    "\t.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b297cf0-c643-47b5-add8-51829f2dd898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-88-97.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.31.88.97:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DS5110/CS5501: my awesome Spark program</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x71b37c9787f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf934cef-b99f-4aa9-8064-5aa7cffb27dd",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d86e9c3-84ae-4303-a893-c4d612b39fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data into X and y\n",
    "X_reg_train = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/X_reg_train.csv')\n",
    "              )\n",
    "X_reg_test = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/X_reg_test.csv')\n",
    "              )\n",
    "X_class_train = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/X_class_train.csv')\n",
    "              )\n",
    "X_class_test = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/X_class_test.csv')\n",
    "              )\n",
    "y_reg_train = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/y_reg_train.csv')\n",
    "              )\n",
    "y_reg_test = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/y_reg_test.csv')\n",
    "              )\n",
    "y_class_train = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/y_class_train.csv')\n",
    "              )\n",
    "y_class_test = (spark.read\n",
    "               .format('csv')\n",
    "               .option('inferSchema', True)\n",
    "               .option('header', True)\n",
    "               .load('hdfs://172.31.88.97:9000/y_class_test.csv')\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0cd3cd-4d2d-4353-90af-a92fd008d9ca",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "# NEEDS TO BE STANDARDIZED ACROSS IMPLEMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ae9f64-85cf-42e3-a1a4-7b3b9265ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = subset_data.randomSplit([0.8, 0.2], seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c7f73-1ff1-409e-8455-66534bb25003",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f432685-1884-418a-825a-6581546873f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='baseFare', predictionCol='prediction', metricName='rmse'\n",
    ")\n",
    "\n",
    "numericCols = ['dateDelta', 'seatsRemaining', 'totalTravelDistance', 'durationSeconds']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "train = assembler.transform(training_data)\n",
    "test = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff1fdf-2c47-4bef-afc1-79ea402805b2",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52be93-14fd-40d3-910f-b4d5378ef956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a Spark object containing\n",
    "# all vectors, called myvecs\n",
    "myvecs.cache()\n",
    " \n",
    "# Create kNN tree locally, and broadcast\n",
    "myvecscollected = myvecs.collect()\n",
    "knnobj = NearestNeighbors().fit(myvecscollected)\n",
    "bc_knnobj = sc.broadcast(knnobj)\n",
    " \n",
    "# Get neighbors for each point, distributedly\n",
    "results = myvecs.map(lambda x: bc_knnobj.value.kneighbors(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c332486-2506-443a-81ee-38e95f732895",
   "metadata": {},
   "source": [
    "## K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4e637-af30-4f79-a324-37a4c1d498ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.7215018401621854\n",
      "Cluster Centers: \n",
      "[2.71430356e+01 6.40547939e+00 1.15242095e+03 1.56616018e+04]\n",
      "[2.73867929e+01 6.00535333e+00 2.16721632e+03 3.69557006e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4d541-14c4-474c-b26d-da3493a3a84d",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce328510-b8d5-4022-912b-e24f0c3286ee",
   "metadata": {},
   "source": [
    "### Timing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc35de8-8200-4aff-8a1d-6623330d827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Fit\n",
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol='baseFare')\n",
    "model = rf.fit(train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='baseFare', predictionCol='prediction', metricName='rmse'\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7788c8-8887-405f-9c96-49ac5c230f90",
   "metadata": {},
   "source": [
    "### Accuracy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb817905-2d22-47f3-b322-8efeadf80952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148.39153072138427"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol='baseFare')\n",
    "model = rf.fit(train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='baseFare', predictionCol='prediction', metricName='rmse'\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1390557-d836-4af8-a7de-1b2b485faddb",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00aff5a1-0e3f-414c-bd34-e310e39cb5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'baseFare', outputCol = 'labelIndex')\n",
    "training_data = label_stringIdx.fit(training_data).transform(training_data)\n",
    "test_data = label_stringIdx.fit(test_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1943ee74-e190-4dff-8168-9785b899488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols = ['dateDelta', 'seatsRemaining', 'totalTravelDistance', 'durationSeconds', 'baseFare']\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "train = assembler.transform(training_data)\n",
    "test = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f19328f-a76e-4c96-af0b-81a0fc86ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 17:31:59 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 17:32:59 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 17:38:44 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "24/04/08 17:38:50 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m ovr \u001b[38;5;241m=\u001b[39m OneVsRest(classifier\u001b[38;5;241m=\u001b[39mlsvc, featuresCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabelIndex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43movr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/ml/classification.py:3586\u001b[0m, in \u001b[0;36mOneVsRest._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier\u001b[38;5;241m.\u001b[39mfit(trainingDataset, paramMap)\n\u001b[1;32m   3584\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numClasses))\n\u001b[0;32m-> 3586\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minheritable_thread_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainSingleClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumClasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handlePersistence:\n\u001b[1;32m   3589\u001b[0m     multiclassLabeled\u001b[38;5;241m.\u001b[39munpersist()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1495:>                                                      (0 + 8) / 20]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "\n",
    "# initiate base classifier\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# initiate one vs. rest classifier\n",
    "ovr = OneVsRest(classifier=lsvc, featuresCol = 'features', labelCol='labelIndex')\n",
    "\n",
    "# Fit model\n",
    "model = ovr.fit(train)\n",
    "\n",
    "# predict\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# initialize evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "# compute classification error\n",
    "accuracy = evlauator.evaluate(predictions)\n",
    "print(f'Test error = {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
